import cv2
import numpy as np
import ncnn
import time
import os
from pathlib import Path
from scipy.optimize import linear_sum_assignment
import queue
from threading import Thread
from collections import defaultdict
from filterpy.kalman import KalmanFilter
from threading import Thread, Event
from datetime import datetime
import requests

class TelegramNotifier(Thread):
    def __init__(self, notification_queue, bot_token, chat_id, stop_event):
        super().__init__()
        self.daemon = True
        self.notification_queue = notification_queue
        self.bot_token = bot_token
        self.chat_id = chat_id
        self.stop_event = stop_event
        print("[INFO] TelegramNotifier ƒë√£ s·∫µn s√†ng.")

    def send_notification(self, vehicle_data):
        """H√†m g·ª≠i ·∫£nh v√† th√¥ng tin ph∆∞∆°ng ti·ªán l√™n Telegram."""
        try:
            # 1. L·∫•y th√¥ng tin c·∫ßn thi·∫øt
            vehicle_id = vehicle_data['id']
            vehicle_class = vehicle_data['final_class']
            image = vehicle_data['image']
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # 2. T·∫°o n·ªôi dung tin nh·∫Øn (caption)
            caption = (
                f"‚úÖ **Ph∆∞∆°ng ti·ªán ƒë∆∞·ª£c nh·∫≠n d·∫°ng**\n\n"
                f"üÜî **ID:** {vehicle_id}\n"
                f"üöó **Lo·∫°i xe:** {vehicle_class}\n"
                f"‚è∞ **Th·ªùi gian:** {timestamp}"
            )

            # 3. Chu·∫©n b·ªã ·∫£nh ƒë·ªÉ g·ª≠i
            # Chuy·ªÉn ƒë·ªïi ·∫£nh t·ª´ ƒë·ªãnh d·∫°ng OpenCV (NumPy array) sang bytes JPEG
            success, encoded_image = cv2.imencode('.jpg', image)
            if not success:
                print("[ERROR] Kh√¥ng th·ªÉ encode ·∫£nh ƒë·ªÉ g·ª≠i ƒëi Telegram.")
                return
            
            image_bytes = encoded_image.tobytes()

            # 4. G·ª≠i request ƒë·∫øn Telegram API
            url = f"https://api.telegram.org/bot{self.bot_token}/sendPhoto"
            files = {'photo': ('vehicle.jpg', image_bytes, 'image/jpeg')}
            payload = {'chat_id': self.chat_id, 'caption': caption, 'parse_mode': 'Markdown'}

            response = requests.post(url, files=files, data=payload, timeout=10) # Timeout 10s
            
            if response.status_code != 200:
                print(f"[ERROR] G·ª≠i th√¥ng b√°o Telegram th·∫•t b·∫°i: {response.text}")

        except Exception as e:
            print(f"[ERROR] ƒê√£ c√≥ l·ªói trong lu·ªìng TelegramNotifier: {e}")

    def run(self):
        while not self.stop_event.is_set():
            try:
                # L·∫•y d·ªØ li·ªáu ph∆∞∆°ng ti·ªán ƒë√£ ho√†n t·∫•t t·ª´ h√†ng ƒë·ª£i
                vehicle_data = self.notification_queue.get(timeout=1)
                
                # G·ª≠i th√¥ng b√°o
                self.send_notification(vehicle_data)

                self.notification_queue.task_done()
            except queue.Empty:
                continue
        
        print("[INFO] TelegramNotifier ƒë√£ d·ª´ng.")

def convert_bbox_to_z(bbox):
    """
    Chuy·ªÉn ƒë·ªïi h·ªôp gi·ªõi h·∫°n [x1, y1, x2, y2] th√†nh d·∫°ng ph√©p ƒëo [cx, cy, a, r]
    trong ƒë√≥ cx, cy l√† t√¢m, a l√† di·ªán t√≠ch, r l√† t·ª∑ l·ªá khung h√¨nh.
    """
    w = bbox[2] - bbox[0]
    h = bbox[3] - bbox[1]
    cx = bbox[0] + 0.5 * w
    cy = bbox[1] + 0.5 * h
    a = w * h
    r = w / float(h)
    return np.array([cx, cy, a, r]).reshape((4, 1))

def convert_x_to_bbox(x):
    """
    Chuy·ªÉn ƒë·ªïi vector tr·∫°ng th√°i th√†nh h·ªôp gi·ªõi h·∫°n.
    Phi√™n b·∫£n "b·∫•t t·ª≠": X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p vector tr·∫°ng th√°i ƒë·∫ßu v√†o ch·ª©a NaN.
    """
    if np.any(np.isnan(x)):
        return np.array([[0, 0, 0, 0]]) 

    s = x[2]
    r = x[3]
    s = max(0, s)
    r = max(1e-4, r)

    w = np.sqrt(s * r)
    h = s / w if w > 1e-4 else 0

    if np.isnan(w) or np.isnan(h) or np.isinf(w) or np.isinf(h):
        return np.array([[0, 0, 0, 0]])

    return np.array([
        x[0] - 0.5 * w,
        x[1] - 0.5 * h,
        x[0] + 0.5 * w,
        x[1] + 0.5 * h
    ]).reshape((1, 4))


class KalmanBoxTracker:
    count = 0
    def __init__(self, bbox):
        self.kf = KalmanFilter(dim_x=7, dim_z=4)
        self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])
        self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])
        self.kf.R[2:,2:] *= 10.
        self.kf.P[4:,4:] *= 1000.
        self.kf.P *= 10.
        self.kf.Q[-1,-1] *= 0.01
        self.kf.Q[4:,4:] *= 0.01
        self.kf.x[:4] = convert_bbox_to_z(bbox)
        
        self.time_since_update = 0
        self.id = KalmanBoxTracker.count
        KalmanBoxTracker.count += 1
        self.history = []
        self.hits = 1
        self.hit_streak = 1
        self.age = 0
        self.class_name = None
        self.color = None
        self.class_counts = defaultdict(int)
        self.best_image_info = {'score': 0.0, 'image': None}

    def update(self, bbox):
        self.time_since_update = 0
        self.history = []
        self.hits += 1
        self.hit_streak += 1
        self.kf.update(convert_bbox_to_z(bbox))

    def predict(self):
        self.kf.predict()
        if self.kf.x[2] < 0:
            self.kf.x[2] = 0
        self.age += 1
        if self.time_since_update > 0:
            self.hit_streak = 0
        self.time_since_update += 1
        self.history.append(convert_x_to_bbox(self.kf.x))
        return self.history[-1]

    def get_state(self):
        return convert_x_to_bbox(self.kf.x)
    
class CameraGrabber(Thread):
    def __init__(self, camera_id, source, input_queue, stop_event, target_fps_for_video=30, processing_width=None):
        super().__init__()
        self.daemon = True
        self.camera_id = camera_id
        self.source = source
        self.input_queue = input_queue
        self.stop_event = stop_event
        self.target_fps_for_video = target_fps_for_video
        self.processing_width = processing_width
        print(f"[INFO] CameraGrabber {self.camera_id} ƒëang kh·ªüi t·∫°o cho ngu·ªìn: {self.source}")

    def run(self):
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            print(f"[ERROR] Kh√¥ng th·ªÉ m·ªü camera {self.camera_id} t·∫°i ngu·ªìn: {self.source}")
            return

        # --- T·ª∞ ƒê·ªòNG PH√ÅT HI·ªÜN LO·∫†I NGU·ªíN ---
        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        is_live_stream = frame_count < 1 # Video files c√≥ frame_count > 0

        if is_live_stream:
            print(f"[INFO] CameraGrabber {self.camera_id}: ƒê√£ ph√°t hi·ªán LIVE STREAM.")
        else:
            print(f"[INFO] CameraGrabber {self.camera_id}: ƒê√£ ph√°t hi·ªán VIDEO FILE. S·∫Ω h√£m t·ªëc ƒë·ªô v·ªÅ ~{self.target_fps_for_video} FPS.")
            frame_delay = 1.0 / self.target_fps_for_video

        while not self.stop_event.is_set():
            if not is_live_stream:
                start_time = time.time()
            
            ret, frame = cap.read()
            if not ret:
                if is_live_stream:
                    print(f"[WARNING] M·∫•t k·∫øt n·ªëi ho·∫∑c k·∫øt th√∫c stream t·ª´ camera {self.camera_id}. Th·ª≠ k·∫øt n·ªëi l·∫°i sau 5s...")
                    cap.release()
                    time.sleep(5)
                    cap = cv2.VideoCapture(self.source)
                    continue
                else:
                    print(f"[INFO] CameraGrabber {self.camera_id}: ƒê√£ x·ª≠ l√Ω h·∫øt video file. D·ª´ng lu·ªìng.")
                    break

            try:
                if self.input_queue.full():
                    self.input_queue.get_nowait() # B·ªè frame c≈© nh·∫•t n·∫øu h√†ng ƒë·ª£i ƒë·∫ßy
                self.input_queue.put((self.camera_id, frame), timeout=1)
            except queue.Full:
                continue
            
            # Ch·ªâ √°p d·ª•ng delay h√£m t·ªëc n·∫øu l√† video file
            if not is_live_stream:
                elapsed_time = time.time() - start_time
                sleep_time = frame_delay - elapsed_time
                if sleep_time > 0:
                    time.sleep(sleep_time)
        
        cap.release()
        print(f"[INFO] CameraGrabber {self.camera_id} ƒë√£ d·ª´ng.")

# D√°n ƒëo·∫°n m√£ n√†y ƒë·ªÉ thay th·∫ø ho√†n to√†n l·ªõp DisplayWorker c≈©
class DisplayWorker(Thread):
    def __init__(self, display_queue, stop_event, fps_stats, ui_enabled=True, min_hits_to_display=3, video_writer_queue=None, output_width=800):
        super().__init__()
        self.daemon = True
        self.display_queue = display_queue
        self.stop_event = stop_event
        self.fps_stats = fps_stats
        self.ui_enabled = ui_enabled
        self.min_hits_to_display = min_hits_to_display
        self.video_writer_queue = video_writer_queue
        self.last_finalized_images = {} 
        self.output_width = output_width # <<< TH√äM M·ªöI: L∆∞u l·∫°i chi·ªÅu r·ªông mong mu·ªën

    def run(self):
        if self.ui_enabled:
            print("[INFO] DisplayWorker ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô UI. Nh·∫•n 'q' tr√™n c·ª≠a s·ªï video ƒë·ªÉ tho√°t.")
        else:
            print("[INFO] DisplayWorker ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô Headless (ch·ªâ t√≠nh FPS).")

        frame_counts = defaultdict(int)
        start_time = time.time()
        
        while not self.stop_event.is_set():
            try:
                # Nh·∫≠n frame g·ªëc (ƒë·ªô ph√¢n gi·∫£i cao) v√† d·ªØ li·ªáu trackers
                cam_id, frame, trackers_to_draw, proc_fps, finalized_vehicles = self.display_queue.get(timeout=1)

                # --- PH·∫¶N 1: T√çNH TO√ÅN V√Ä V·∫º L√äN FRAME G·ªêC (GI·ªÆ NGUY√äN) ---
                if finalized_vehicles:
                    last_vehicle = finalized_vehicles[-1] 
                    if 'image' in last_vehicle and last_vehicle['image'] is not None:
                        self.last_finalized_images[cam_id] = last_vehicle['image']
                
                frame_counts[cam_id] += 1
                
                current_time = time.time()
                elapsed_time = current_time - start_time
                if elapsed_time >= 1.0:
                    for cid, count in frame_counts.items():
                        self.fps_stats[cid] = count / elapsed_time
                    frame_counts.clear()
                    start_time = current_time
                
                display_fps = self.fps_stats.get(cam_id, 0.0)

                # V·∫Ω tracker l√™n frame g·ªëc
                for tracker in trackers_to_draw:
                    box = tracker.get_state()[0]
                    if tracker.hits >= self.min_hits_to_display and not np.any(np.isnan(box)):
                        x1, y1, x2, y2 = box.astype(int)
                        label = f"ID {tracker.id}: {tracker.class_name}"
                        pt1, pt2 = (x1, y1), (x2, y2)
                        color = tracker.color if tracker.color is not None else (0, 0, 255)
                        cv2.rectangle(frame, pt1, pt2, color, 2)
                        cv2.putText(frame, label, (pt1[0], pt1[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                # V·∫Ω th√¥ng tin FPS l√™n frame g·ªëc
                cv2.putText(frame, f"Proc FPS: {proc_fps:.2f} (Tracking)", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                cv2.putText(frame, f"System FPS: {display_fps:.2f} (Display)", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # V·∫Ω thumbnail l√™n frame g·ªëc
                if cam_id in self.last_finalized_images:
                    saved_img = self.last_finalized_images[cam_id]
                    thumb_h, thumb_w = 100, 100
                    try:
                        thumbnail = cv2.resize(saved_img, (thumb_w, thumb_h), interpolation=cv2.INTER_AREA)
                        y_offset, x_offset = 90, 10
                        if y_offset + thumb_h < frame.shape[0] and x_offset + thumb_w < frame.shape[1]:
                            cv2.putText(frame, "Last Vehicle Saved:", (x_offset, y_offset - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                            frame[y_offset:y_offset + thumb_h, x_offset:x_offset + thumb_w] = thumbnail
                    except cv2.error:
                        pass
                frame_to_output = frame
                if frame.shape[1] > self.output_width:
                    h, w, _ = frame.shape
                    ratio = self.output_width / w
                    new_h = int(h * ratio)
                    frame_to_output = cv2.resize(frame, (self.output_width, new_h), interpolation=cv2.INTER_AREA)
                if self.video_writer_queue is not None:
                    try:
                        self.video_writer_queue.put((cam_id, frame_to_output.copy()), block=False)
                    except queue.Full:
                        pass
                if self.ui_enabled:
                    cv2.imshow(f"Camera {cam_id}", frame_to_output)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        self.stop_event.set()
                        break
            except queue.Empty:
                if self.ui_enabled and cv2.waitKey(1) & 0xFF == ord('q'):
                     self.stop_event.set()
                     break
                continue
        
        if self.ui_enabled:
            cv2.destroyAllWindows()
        print("[INFO] DisplayWorker ƒë√£ d·ª´ng.")

class InferenceWorker(Thread):
    """
    M·ªôt lu·ªìng duy nh·∫•t nh·∫≠n c√°c frame t·ª´ h√†ng ƒë·ª£i, th·ª±c hi·ªán suy lu·∫≠n theo l√¥,
    v√† ƒë∆∞a k·∫øt qu·∫£ v√†o h√†ng ƒë·ª£i ƒë·∫ßu ra.
    """
    def __init__(self, model, input_queue, output_queue, stop_event, batch_size=2):
        super().__init__()
        self.daemon = True
        self.model = model
        self.input_queue = input_queue
        self.output_queue = output_queue
        self.stop_event = stop_event
        self.batch_size = batch_size
        print(f"[INFO] InferenceWorker kh·ªüi t·∫°o v·ªõi batch size = {self.batch_size}")

    def run(self):
        while not self.stop_event.is_set():
            batch_data = []
            try:
                # C·ªë g·∫Øng l·∫•y m·ªôt l√¥ d·ªØ li·ªáu
                for _ in range(self.batch_size):
                    # L·∫•y item ƒë·∫ßu ti√™n v·ªõi timeout d√†i h∆°n m·ªôt ch√∫t
                    # C√°c item sau l·∫•y v·ªõi timeout ng·∫Øn ƒë·ªÉ x·ª≠ l√Ω l√¥ kh√¥ng ho√†n ch·ªânh
                    timeout = 0.5 if len(batch_data) > 0 else 1.0
                    data = self.input_queue.get(timeout=timeout)
                    batch_data.append(data)
            except queue.Empty:
                # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu sau timeout, ti·∫øp t·ª•c v√≤ng l·∫∑p
                if not batch_data:
                    continue

            # T√°ch id v√† frame ra kh·ªèi l√¥
            cam_ids, frames = zip(*batch_data)

            # Th·ª±c hi·ªán suy lu·∫≠n tr√™n to√†n b·ªô l√¥
            # ƒê√¢y l√† ƒëi·ªÉm m·∫•u ch·ªët: 1 l·∫ßn g·ªçi cho nhi·ªÅu ·∫£nh
            all_detections = self.model.detect_batch(list(frames))

            # ƒê·∫©y k·∫øt qu·∫£ v√†o h√†ng ƒë·ª£i ƒë·∫ßu ra
            for i in range(len(batch_data)):
                try:
                    self.output_queue.put((cam_ids[i], frames[i], all_detections[i]), timeout=1)
                except queue.Full:
                    continue

        print("[INFO] InferenceWorker ƒë√£ d·ª´ng.")



class StatsPrinter(Thread):
    """
    M·ªôt lu·ªìng si√™u nh·∫π chuy√™n d·ª•ng ƒë·ªÉ in t√≥m t·∫Øt th·ªëng k√™ FPS ra console
    m·ªôt c√°ch ƒë·ªãnh k·ª≥, ph√π h·ª£p cho m√¥i tr∆∞·ªùng headless.
    """
    def __init__(self, fps_stats, stop_event, print_interval=2.0):
        super().__init__()
        self.daemon = True
        self.fps_stats = fps_stats
        self.stop_event = stop_event
        self.print_interval = print_interval # In th·ªëng k√™ sau m·ªói 2 gi√¢y

    def run(self):
        print("[INFO] StatsPrinter b·∫Øt ƒë·∫ßu ch·∫°y.")
        while not self.stop_event.is_set():
            # ƒê·ª£i cho ƒë·∫øn k·ª≥ in ti·∫øp theo
            if self.stop_event.wait(self.print_interval):
                # N·∫øu nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng trong l√∫c ƒë·ª£i, tho√°t ngay
                break

            # T·∫°o chu·ªói t√≥m t·∫Øt FPS
            summary_parts = []
            # S·∫Øp x·∫øp theo cam_id ƒë·ªÉ th·ª© t·ª± in ra lu√¥n ·ªïn ƒë·ªãnh
            sorted_cam_ids = sorted(self.fps_stats.keys())

            for cam_id in sorted_cam_ids:
                fps = self.fps_stats.get(cam_id, 0.0)
                summary_parts.append(f"Cam {cam_id}: {fps:.2f} FPS")
            
            summary_string = " | ".join(summary_parts)
            
            # In ra console v√† s·ª≠ d·ª•ng '\r' ƒë·ªÉ ghi ƒë√® l√™n d√≤ng c≈©
            # flush=True ƒë·∫£m b·∫£o log ƒë∆∞·ª£c ghi ra ngay l·∫≠p t·ª©c trong container
            if summary_string:
                print(f"\r[STATS] {summary_string}", end="", flush=True)

        print("\n[INFO] StatsPrinter ƒë√£ d·ª´ng.")

class VideoWriterWorker(Thread):
    """
    PHI√äN B·∫¢N N√ÇNG C·∫§P: Logic d·ª´ng lu·ªìng m·∫°nh m·∫Ω h∆°n.
    """
    def __init__(self, writer_queue, stop_event, output_dir="output_videos", fps=20.0):
        super().__init__()
        self.daemon = True
        self.writer_queue = writer_queue
        self.stop_event = stop_event
        self.output_dir = output_dir
        self.fps = fps
        self.video_writers = {}
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"[INFO] VideoWriterWorker ƒë√£ s·∫µn s√†ng, video s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}")

    def run(self):
        try:
            while True: # <<< THAY ƒê·ªîI: V√≤ng l·∫∑p v√¥ t·∫≠n
                try:
                    # Ch·ªù d·ªØ li·ªáu, nh∆∞ng kh√¥ng timeout qu√° l√¢u ƒë·ªÉ c√≥ th·ªÉ ki·ªÉm tra stop_event
                    data = self.writer_queue.get(timeout=0.5) 
                    
                    # <<< LOGIC M·ªöI: Ki·ªÉm tra "Poison Pill" >>>
                    if data is None:
                        # Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng, tho√°t kh·ªèi v√≤ng l·∫∑p
                        break

                    cam_id, frame = data
                    
                    if cam_id not in self.video_writers:
                        h, w, _ = frame.shape
                        fourcc = cv2.VideoWriter_fourcc(*'XVID')
                        file_path = os.path.join(self.output_dir, f"output_cam_{cam_id}.avi")
                        print(f"[INFO] T·∫°o file video cho Camera {cam_id} t·∫°i: {file_path}")
                        self.video_writers[cam_id] = cv2.VideoWriter(file_path, fourcc, self.fps, (w, h))
                    
                    self.video_writers[cam_id].write(frame)
                    self.writer_queue.task_done() # B√°o cho queue bi·∫øt item ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω

                except queue.Empty:
                    # N·∫øu kh√¥ng c√≥ frame, ki·ªÉm tra xem c√≥ n√™n d·ª´ng kh√¥ng
                    if self.stop_event.is_set():
                        break
                    continue
        finally:
            # D·ªçn d·∫πp: ƒê√≥ng t·∫•t c·∫£ c√°c file video khi lu·ªìng d·ª´ng
            print("[INFO] VideoWriterWorker ƒëang ƒë√≥ng c√°c file video...")
            for writer in self.video_writers.values():
                writer.release()
            print("[INFO] VideoWriterWorker ƒë√£ d·ª´ng v√† ƒë√≥ng file th√†nh c√¥ng.")

    def close(self):
        # C√≥ th·ªÉ g·ªçi h√†m n√†y t·ª´ b√™n ngo√†i ƒë·ªÉ ch·ªß ƒë·ªông ƒë√≥ng
        self.stop_event.set()

# L·ªõp YOLOv8NCNN gi·ªØ nguy√™n, kh√¥ng c·∫ßn thay ƒë·ªïi
class YOLOv8NCNN:
    def __init__(self, param_path, bin_path, conf_threshold=0.25, iou_threshold=0.45, per_class_conf=None):
        self.input_size = 640 
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.per_class_conf = per_class_conf if per_class_conf is not None else {}
        self.net = ncnn.Net()

        # --- T·ªêI ∆ØU CHO RASPBERRY PI 5 ---
        print("[INFO] Applying NCNN optimizations for ARM CPU...")
        self.net.opt.use_vulkan_compute = False  # Ch·∫Øc ch·∫Øn d√πng CPU
        self.net.opt.use_winograd_convolution = True
        self.net.opt.use_sgemm_convolution = True
        # B·ªè comment d√≤ng d∆∞·ªõi n·∫øu b·∫°n ƒë√£ l∆∞·ª£ng t·ª≠ h√≥a model th√†nh c√¥ng
        # self.net.opt.use_int8_inference = True
        self.net.opt.use_fp16_packed = True
        self.net.opt.use_fp16_storage = True
        self.net.opt.use_fp16_arithmetic = True
        self.net.opt.use_packing_layout = True
        self.net.opt.num_threads = 3
        
        # Load model
        self.net.load_param(param_path)
        self.net.load_model(bin_path)
        
        # COCO class names
        self.class_names = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]
        np.random.seed(112006) # ƒê·∫£m b·∫£o m√†u s·∫Øc kh√¥ng thay ƒë·ªïi m·ªói l·∫ßn ch·∫°y
        self.colors = [tuple(np.random.randint(0, 255, 3).tolist()) for _ in range(len(self.class_names))]
    
    def preprocess(self, img):
        """Preprocess image for YOLOv8"""
        # Resize with letterbox
        h, w = img.shape[:2]
        scale = min(self.input_size / h, self.input_size / w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        # Create padded image
        padded = np.full((self.input_size, self.input_size, 3), 114, dtype=np.uint8)
        pad_h = (self.input_size - new_h) // 2
        pad_w = (self.input_size - new_w) // 2
        padded[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized
        
        return padded, scale, pad_h, pad_w
    
    def postprocess(self, output, scale, pad_h, pad_w, orig_shape):
        """Postprocess YOLOv8 output (Optimized Version)"""
        predictions = output[0].T
        
        boxes = predictions[:, :4]  # cx, cy, w, h
        scores = predictions[:, 4:]
        
        # 1. L·ªçc b·∫±ng ng∆∞·ª°ng tin c·∫≠y (Vectorized)
        class_ids = np.argmax(scores, axis=1)
        confidences = np.max(scores, axis=1)
        
        # L·ªçc nh·ªØng detection c√≥ confidence d∆∞·ªõi ng∆∞·ª°ng chung
        mask = confidences > self.conf_threshold
        boxes = boxes[mask]
        confidences = confidences[mask]
        class_ids = class_ids[mask]
        
        if boxes.shape[0] == 0:
            return []

        # 2. Chuy·ªÉn ƒë·ªïi h·ªôp t·ª´ (cx, cy, w, h) sang (x1, y1, x2, y2)
        x1 = boxes[:, 0] - boxes[:, 2] / 2
        y1 = boxes[:, 1] - boxes[:, 3] / 2
        x2 = boxes[:, 0] + boxes[:, 2] / 2
        y2 = boxes[:, 1] + boxes[:, 3] / 2
        
        # 3. Scale l·∫°i t·ªça ƒë·ªô v·ªÅ ·∫£nh g·ªëc
        x1 = (x1 - pad_w) / scale
        y1 = (y1 - pad_h) / scale
        x2 = (x2 - pad_w) / scale
        y2 = (y2 - pad_h) / scale
        
        # Clip t·ªça ƒë·ªô
        x1 = np.clip(x1, 0, orig_shape[1])
        y1 = np.clip(y1, 0, orig_shape[0])
        x2 = np.clip(x2, 0, orig_shape[1])
        y2 = np.clip(y2, 0, orig_shape[0])
        
        boxes_for_nms = np.column_stack([x1, y1, x2 - x1, y2 - y1]).astype(int)
        keep_indices = cv2.dnn.NMSBoxes(boxes_for_nms.tolist(), confidences.tolist(), self.conf_threshold, self.iou_threshold)
    
        if isinstance(keep_indices, np.ndarray):
            keep_indices = keep_indices.flatten()
        else: 
            return []

        allowed_classes = {'car', 'motorcycle', 'bus', 'truck'}
        
        results = []
        for idx in keep_indices:
            class_id = class_ids[idx]
            class_name = self.class_names[class_id]
            
            final_threshold = self.per_class_conf.get(class_name, self.conf_threshold)

            if class_name in allowed_classes and confidences[idx] >= final_threshold:
                box = np.array([x1[idx], y1[idx], x2[idx], y2[idx]])
                results.append({
                    'box': box.astype(int),
                    'confidence': float(confidences[idx]),
                    'class_id': int(class_id),
                    'class_name': class_name
                })
                
        return results

    def detect(self, img, debug=False):
        orig_shape = img.shape[:2]
        
        preprocessed, scale, pad_h, pad_w = self.preprocess(img)
        
        mat_in = ncnn.Mat.from_pixels(
            preprocessed,
            ncnn.Mat.PixelType.PIXEL_BGR2RGB,
            self.input_size,
            self.input_size
        )
        
        mat_in.substract_mean_normalize([0.0, 0.0, 0.0], [1/255.0, 1/255.0, 1/255.0])
        
        ex = self.net.create_extractor()
        ex.input("in0", mat_in)
        
        ret, mat_out = ex.extract("out0")
        
        output = np.array(mat_out)
        
        if len(output.shape) == 1:
            output = output.reshape(84, -1)[np.newaxis, :, :]
        elif len(output.shape) == 2:
            output = output[np.newaxis, :, :]
            
        detections = self.postprocess(output, scale, pad_h, pad_w, orig_shape)
        
        return detections
    def detect_batch(self, images):
        """
        Th·ª±c hi·ªán ph√°t hi·ªán tr√™n m·ªôt l√¥ (danh s√°ch) c√°c ·∫£nh.
        Tr·∫£ v·ªÅ m·ªôt danh s√°ch c√°c k·∫øt qu·∫£ detection.
        """
        results = []
        for img in images:
            # G·ªçi h√†m detect cho t·ª´ng ·∫£nh
            detections = self.detect(img)
            results.append(detections)
        return results

class VideoWriterAsync:
    def __init__(self, name, fourcc, fps, frame_size):
        self.q = queue.Queue()
        self.writer = cv2.VideoWriter(name, fourcc, fps, frame_size)
        self.thread = Thread(target=self.run, daemon=True)
        self.thread.start()

    def run(self):
        while True:
            frame = self.q.get()
            if frame is None: # T√≠n hi·ªáu k·∫øt th√∫c
                break
            self.writer.write(frame)
        self.writer.release()

    def write(self, frame):
        self.q.put(frame)

    def release(self):
        self.q.put(None) # G·ª≠i t√≠n hi·ªáu k·∫øt th√∫c
        self.thread.join() # Ch·ªù thread k·∫øt th√∫c


def calculate_iou(boxA, boxB):
    # X√°c ƒë·ªãnh t·ªça ƒë·ªô c·ªßa v√πng giao nhau (intersection)
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    # T√≠nh di·ªán t√≠ch v√πng giao nhau
    interArea = max(0, xB - xA) * max(0, yB - yA)

    # T√≠nh di·ªán t√≠ch c·ªßa t·ª´ng h·ªôp
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    # T√≠nh di·ªán t√≠ch v√πng h·ª£p nh·∫•t (union)
    unionArea = float(boxAArea + boxBArea - interArea)

    # T√≠nh IoU
    iou = interArea / unionArea if unionArea > 0 else 0
    return iou

def calculate_iou_matrix(boxesA, boxesB):
    if boxesA.size == 0 or boxesB.size == 0:
        return np.empty((boxesA.shape[0], boxesB.shape[0]))
    boxesA = np.expand_dims(boxesA, axis=1)
    boxesB = np.expand_dims(boxesB, axis=0)
    xA = np.maximum(boxesA[..., 0], boxesB[..., 0])
    yA = np.maximum(boxesA[..., 1], boxesB[..., 1])
    xB = np.minimum(boxesA[..., 2], boxesB[..., 2])
    yB = np.minimum(boxesA[..., 3], boxesB[..., 3])
    interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)
    boxAArea = (boxesA[..., 2] - boxesA[..., 0]) * (boxesA[..., 3] - boxesA[..., 1])
    boxBArea = (boxesB[..., 2] - boxesB[..., 0]) * (boxesB[..., 3] - boxesB[..., 1])
    
    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / unionArea
    iou[unionArea == 0] = 0
    
    return iou

def associate_detections_to_trackers(tracked_boxes, detections_boxes, iou_threshold=0.3):
    tracked_boxes = np.asarray(tracked_boxes)
    detections_boxes = np.asarray(detections_boxes)

    if tracked_boxes.size == 0 or detections_boxes.size == 0:
        return (
            np.empty((0, 2), dtype=int),
            np.arange(len(detections_boxes)),
            np.arange(len(tracked_boxes)),
        )

    iou_matrix = calculate_iou_matrix(tracked_boxes, detections_boxes)
    cost_matrix = 1 - iou_matrix
    cost_matrix[np.isnan(cost_matrix)] = 1.0
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    
    matches = []
    matched_trackers_indices = set()
    matched_detections_indices = set()

    for r, c in zip(row_ind, col_ind):
        if iou_matrix[r, c] >= iou_threshold:
            matches.append([r, c])
            matched_trackers_indices.add(r)
            matched_detections_indices.add(c)
    
    all_trackers_indices = set(range(tracked_boxes.shape[0]))
    all_detections_indices = set(range(detections_boxes.shape[0]))
    
    unmatched_trackers = np.array(list(all_trackers_indices - matched_trackers_indices))
    unmatched_detections = np.array(list(all_detections_indices - matched_detections_indices))

    return np.array(matches, dtype=int), unmatched_detections, unmatched_trackers

def calculate_quality_score(frame, box, confidence):
    x1, y1, x2, y2 = box
    if x1 >= x2 or y1 >= y2:
        return 0.0

    # 1. Tr·ªçng s·ªë cho ƒë·ªô tin c·∫≠y (quan tr·ªçng nh·∫•t)
    score = confidence * 1.5

    # 2. Tr·ªçng s·ªë cho di·ªán t√≠ch
    box_area = (x2 - x1) * (y2 - y1)
    frame_area = frame.shape[0] * frame.shape[1]
    normalized_area = box_area / frame_area
    score += normalized_area * 0.5

    # 3. Tr·ªçng s·ªë cho v·ªã tr√≠ trung t√¢m (c√†ng g·∫ßn trung t√¢m c√†ng t·ªët)
    frame_center_x, frame_center_y = frame.shape[1] / 2, frame.shape[0] / 2
    box_center_x, box_center_y = (x1 + x2) / 2, (y1 + y2) / 2
    dist_from_center = np.sqrt((frame_center_x - box_center_x)**2 + (frame_center_y - box_center_y)**2)
    max_dist = np.sqrt(frame_center_x**2 + frame_center_y**2)
    centrality_score = 1.0 - (dist_from_center / max_dist)
    score += centrality_score * 1.0

    # 4. (N√¢ng cao) Tr·ªçng s·ªë cho ƒë·ªô s·∫Øc n√©t (ch·ªëng m·ªù)
    crop = frame[y1:y2, x1:x2]
    # if crop.size > 0:
    #     gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)
    #     sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
    #     normalized_sharpness = min(sharpness / 2000.0, 1.0) # C·∫ßn tinh ch·ªânh ng∆∞·ª°ng 2000
    #     score += normalized_sharpness * 0.7

    return score

def finalize_vehicle_data_from_tracker(tracker):
    if not tracker.class_counts:
        return None
    final_class = max(tracker.class_counts, key=tracker.class_counts.get)
    best_image = tracker.best_image_info['image']
    if best_image is not None:
        return {
            'id': tracker.id,
            'final_class': final_class,
            'class_details': dict(tracker.class_counts),
            'image': best_image,
            'last_box': tracker.get_state()[0].astype(int)
        }
    return None

def save_finalized_results(finalized_vehicles, output_dir="finalized_vehicles"):

    if not finalized_vehicles:
        return
        
    os.makedirs(output_dir, exist_ok=True)
    
    for vehicle in finalized_vehicles:
        file_name = f"ID_{vehicle['id']}_{vehicle['final_class']}.jpg"
        file_path = os.path.join(output_dir, file_name)
        cv2.imwrite(file_path, vehicle['image'])

def main(ENABLE_UI=True, SAVE_VIDEO=False,
        STATS_PRINT_INTERVAL=1.0,  PARAM_PATH=str, 
        BIN_PATH=str, CAMERA_SOURCES=None, PROCESSING_WIDTH=800,
        DETECTION_INTERVAL=4, IOU_THRESHOLD_TRACKING=0.3,
        MAX_AGE=15, MIN_HITS_TO_DISPLAY = 2, MIN_HITS_FOR_CLASSIFICATION = 10,
        DEFAULT_CONF_THRESHOLD=0.3, PER_CLASS_THRESHOLDS=None, BOT_TOKEN=None, CHAT_ID=None):

    print("[INFO] loading YOLO NCNN...")
    model = YOLOv8NCNN(PARAM_PATH, BIN_PATH, conf_threshold=DEFAULT_CONF_THRESHOLD, iou_threshold=0.5, per_class_conf=PER_CLASS_THRESHOLDS)
    print("[INFO] model loaded.")

    raw_frame_queue = queue.Queue(maxsize=len(CAMERA_SOURCES) * 2)
    inference_input_queue = queue.Queue(maxsize=2)
    inference_output_queue = queue.Queue(maxsize=2)
    display_queue = queue.Queue(maxsize=len(CAMERA_SOURCES) * 2)
    video_writer_queue = queue.Queue(maxsize=len(CAMERA_SOURCES) * 5) if SAVE_VIDEO else None
    notification_queue = queue.Queue(maxsize=20)

    stop_event = Event()
    threads = []
    fps_stats = {} 

    
    # 1. C√°c lu·ªìng CameraGrabber
    for i, source in enumerate(CAMERA_SOURCES):
        grabber = CameraGrabber(i, source, raw_frame_queue, stop_event, processing_width=PROCESSING_WIDTH)
        threads.append(grabber)

    # 2. M·ªôt lu·ªìng InferenceWorker
    inference_worker = InferenceWorker(model, inference_input_queue, inference_output_queue, stop_event, batch_size=1)
    threads.append(inference_worker)

    display_worker = DisplayWorker(
        display_queue, stop_event, fps_stats, 
        ui_enabled=ENABLE_UI, 
        min_hits_to_display=MIN_HITS_TO_DISPLAY,
        video_writer_queue=video_writer_queue,
        output_width=PROCESSING_WIDTH
    )
    threads.append(display_worker)

    stats_printer = StatsPrinter(fps_stats, stop_event, print_interval=STATS_PRINT_INTERVAL)
    threads.append(stats_printer)

    if SAVE_VIDEO:
        video_writer_worker = VideoWriterWorker(video_writer_queue, stop_event, fps=20.0)
        threads.append(video_writer_worker)

    if BOT_TOKEN != "YOUR_BOT_TOKEN" and CHAT_ID != "YOUR_CHAT_ID":
        telegram_worker = TelegramNotifier(notification_queue, BOT_TOKEN, CHAT_ID, stop_event)
        threads.append(telegram_worker)
    else:
        print("[WARNING] BOT_TOKEN v√† CHAT_ID ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Lu·ªìng Telegram s·∫Ω kh√¥ng ch·∫°y.")

    print(f"[INFO] run {len(threads)} thread...")
    for t in threads:
        t.start()

    trackers_per_camera = {i: [] for i in range(len(CAMERA_SOURCES))}
    frame_counters = {i: 0 for i in range(len(CAMERA_SOURCES))}
    KalmanBoxTracker.count = 0

    all_finalized_vehicles = []

    try:
        while not stop_event.is_set():
            try:
                cam_id, frame = raw_frame_queue.get(timeout=1)
            except queue.Empty:
                continue

            finalized_this_frame = []

            start_time = time.time()
            current_trackers = trackers_per_camera[cam_id]
            
            predicted_boxes = [tracker.predict()[0] for tracker in current_trackers]
            
            if frame_counters[cam_id] % DETECTION_INTERVAL == 0:
                # L·∫•y k·∫øt qu·∫£ detection
                inference_input_queue.put((cam_id, frame))
                processed_cam_id, _, detections = inference_output_queue.get(timeout=0.5)
                
                detection_boxes = [d['box'] for d in detections]
                
                matches, unmatched_dets, unmatched_trks = \
                    associate_detections_to_trackers(np.array(predicted_boxes), np.array(detection_boxes), iou_threshold=IOU_THRESHOLD_TRACKING)

                for match in matches:
                    tracker_idx, detection_idx = match[0], match[1]
                    det = detections[detection_idx]
                    tracker = current_trackers[tracker_idx]
                    
                    tracker.update(det['box'])
                    tracker.class_name = det['class_name']
                    tracker.color = model.colors[det['class_id']]
                    
                    tracker.class_counts[det['class_name']] += 1
                    
                    # 2. T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng v√† c·∫≠p nh·∫≠t ·∫£nh t·ªët nh·∫•t
                    box = det['box'].astype(int)
                    score = calculate_quality_score(frame, box, det['confidence'])
                    
                    if score > tracker.best_image_info['score']:
                        tracker.best_image_info['score'] = score
                        # Crop v√† l∆∞u l·∫°i h√¨nh ·∫£nh
                        x1, y1, x2, y2 = box
                        tracker.best_image_info['image'] = frame[y1:y2, x1:x2].copy()

                for d_idx in unmatched_dets:
                    det = detections[d_idx]
                    new_tracker = KalmanBoxTracker(det['box'])
                    new_tracker.class_name = det['class_name']
                    new_tracker.color = model.colors[det['class_id']]
                    # C·∫≠p nh·∫≠t l·∫ßn ƒë·∫ßu cho tracker m·ªõi
                    new_tracker.class_counts[det['class_name']] += 1
                    current_trackers.append(new_tracker)
                
                next_frame_trackers = []
                
                for i, t in enumerate(current_trackers):
                    is_unmatched_and_old = i in unmatched_trks and t.time_since_update > MAX_AGE
                    
                    if not is_unmatched_and_old:
                        next_frame_trackers.append(t)
                    else:
                        if t.hits >= MIN_HITS_FOR_CLASSIFICATION:
                            final_data = finalize_vehicle_data_from_tracker(t)
                            if final_data:
                                finalized_this_frame.append(final_data)
                                all_finalized_vehicles.append(final_data)
                                try:
                                    notification_queue.put(final_data, block=False)
                                except queue.Full:
                                    print("[WARNING] H√†ng ƒë·ª£i th√¥ng b√°o Telegram b·ªã ƒë·∫ßy, b·ªè qua th√¥ng b√°o.")
                                    pass
                trackers_per_camera[cam_id] = next_frame_trackers
                if finalized_this_frame:
                    save_finalized_results(finalized_this_frame)
            
            processing_time = time.time() - start_time
            proc_fps = 1.0 / processing_time if processing_time > 0 else 0

            try:
                display_queue.put((cam_id, frame, trackers_per_camera[cam_id], proc_fps, finalized_this_frame), timeout=1)
            except queue.Full:
                pass

            frame_counters[cam_id] += 1

    except KeyboardInterrupt:
        print("\n[INFO] catch (Ctrl+C)...")
    finally:
        print("[INFO] ƒêang d·ª´ng c√°c lu·ªìng, vui l√≤ng ƒë·ª£i...")
        if not stop_event.is_set():
            stop_event.set()
        time.sleep(1) 
        if SAVE_VIDEO:
            if not video_writer_queue.full():
                 video_writer_queue.put(None)
        for t in threads:
            t.join(timeout=5.0)

        print("\n--- T·ªîNG K·∫æT ---")
        print(f"T·ªïng s·ªë ph∆∞∆°ng ti·ªán ƒë√£ ƒë∆∞·ª£c theo d√µi v√† ho√†n t·∫•t: {len(all_finalized_vehicles)}")
        save_finalized_results(all_finalized_vehicles)

        print("[INFO] H·ªá th·ªëng ƒë√£ d·ª´ng ho√†n to√†n.")

if __name__ == "__main__":

    "rtsp://admin:clbAI_2021@192.168.16.224"
    """http://192.168.28.78:8080/14d87061586c7ce87be314ac1bf7db6e/hls/gqbb9Lhhcu/0fceca1c4aa34bd3a87853f47f841cc9/s.m3u8"""
    """http://192.168.28.78:8080/3584d423c76ee0c27b9091351435ac4a/hls/gqbb9Lhhcu/ee79SkHP6y/s.m3u8"""

    BOT_TOKEN="7706726930:AAE0gDgfaNIHvkoZzqNrIMAuZrp9dTuw8KA"
    CHAT_ID="7787124769"
    # --- C·∫•u h√¨nh Chung ---
    ENABLE_UI = True
    STATS_PRINT_INTERVAL = 1
    SAVE_VIDEO = False
    # --- C·∫•u h√¨nh M√¥ h√¨nh v√† Ngu·ªìn Camera ---
    PARAM_PATH = "models/yolo11n_ncnn_model/model.ncnn.param"
    BIN_PATH = "models/yolo11n_ncnn_model/model.ncnn.bin"
    CAMERA_SOURCES = [
        # "4.mp4",
        "http://192.168.28.78:8080/14d87061586c7ce87be314ac1bf7db6e/hls/gqbb9Lhhcu/0fceca1c4aa34bd3a87853f47f841cc9/s.m3u8",
        # "assets/4.mp4"
    ]
    
    # --- C·∫•u h√¨nh Hi·ªáu NƒÉng v√† Tracking ---
    PROCESSING_WIDTH = 800
    DETECTION_INTERVAL = 3
    IOU_THRESHOLD_TRACKING = 0.3    
    MAX_AGE = 10        
    MIN_HITS_TO_DISPLAY = 2     
    MIN_HITS_FOR_CLASSIFICATION = 10
    # --- C·∫•u h√¨nh Ng∆∞·ª°ng Tin C·∫≠y c·ªßa Model ---
    DEFAULT_CONF_THRESHOLD = 0.3
    PER_CLASS_THRESHOLDS = {
        'car': 0.35, 'motorcycle': 0.15,
        'bus': 0.5, 'truck': 0.4, 'person':0.1
    }
    main(ENABLE_UI, SAVE_VIDEO,
        STATS_PRINT_INTERVAL, PARAM_PATH, 
        BIN_PATH, CAMERA_SOURCES, PROCESSING_WIDTH,
        DETECTION_INTERVAL, IOU_THRESHOLD_TRACKING,
        MAX_AGE, MIN_HITS_TO_DISPLAY, MIN_HITS_FOR_CLASSIFICATION,
        DEFAULT_CONF_THRESHOLD, PER_CLASS_THRESHOLDS, BOT_TOKEN, CHAT_ID)